# ğ“º Gebeya Lens ğ“º

## Decoding Ethiopiaâ€™s Telegram E-Commerce Chaos with AI/ML

This project extracts structured product data (Product, Price, Location) from Amharic Telegram channels and analyzes vendor performance for financial services.

## ğŸ”§ Tasks Overview
1. Scrape Telegram channels with Amharic vendor posts
2. Preprocess and normalize Amharic text
3. Label subset in CoNLL format
4. Fine-tune NER models (XLM-R, mBERT, etc.)
5. Compare and interpret models (SHAP, LIME)
6. Score vendors using extracted entities + metadata

## ğŸ“ Structure
```
ğŸ“¦ gebeyalens
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ processed/
â”‚ â””â”€â”€ labeled/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ scripts/
â”œâ”€â”€ models/
â”œâ”€â”€ reports/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py
â””â”€â”€ README.md
```

## ğŸ‘¨â€ğŸ’» Tech Stack
- Python, Telethon
- HuggingFace Transformers
- Amharic NLP tools
- SHAP, LIME for interpretability

---
## ğŸ›  Task 1: Scraping Amharic E-Commerce Telegram Channels

### ğŸ¯ Objective  
Collect e-commerce posts from Amharic Telegram channels to build a raw dataset for entity recognition. Focused on extracting product details, pricing, locations, contacts, and delivery info.

### ğŸš€ Approach  

- **Tool**: `Telethon` for asynchronous interaction with the Telegram API.  
- **Channels Scraped**: 19 public e-commerce channels (e.g., `@ZemenExpress`, `@AwasMart`, `@belaclassic`, `@Leyueqa`)  
- **Excluded**: `@ethio_brand_collection` (private/inactive)

### ğŸ“Œ Data Fields  

| Field        | Description |
|--------------|-------------|
| `channel`    | Channel name (e.g., belaclassic) |
| `message_id` | Post ID (e.g., 1645) |
| `text`       | Post content (Amharic/English) |
| `timestamp`  | Date/time of post |
| `views`      | View count |
| `media_path` | Image file path (if available) |

### ğŸ“¤ Output  

- **File**: `data/raw/scraped.csv` (488 posts)  
- **Images**: Stored in `data/raw/photos/` (~60% posts include images)

### ğŸ“Š Results  

- **Total Posts**: 488  
- **Channels Covered**: 19  
- **No missing text**, **No duplicate rows**  
- **Top Channels**:  
  - `belaclassic` (50 posts)  
  - `qnashcom` (40 posts)  
  - `sinayelj` (11 posts)  

### ğŸ” Content Patterns  

- **Products**: Mixed-language descriptions (e.g., "PUMA SPIRIT", "á‹¨áŒ¨á‰…áˆ‹ áˆ…áƒáŠ“á‰µ áˆ›á‰€áŠá‹«")  
- **Prices**:  
  - â€œá‹‹áŒ‹á¦ 2,000 á‰¥áˆ­â€  
  - â€œPRICE 4900 á‰¥áˆ­â€  
- **Locations**:  
  - Amharic: â€œáˆ˜áŒˆáŠ“áŠ›â€  
  - English: â€œMexicoâ€  
- **Contacts**:  
  - Phone numbers: â€œ0944222069â€  
  - Telegram handles: â€œ@zemencallcenterâ€  
- **Delivery**: Terms like â€œáŠ¨áŠáƒ á‹²áˆŠá‰¨áˆªâ€, â€œá‰ áˆá‰°áˆ¨áŠá‰½â€

---

## âš ï¸ Challenges  

| Challenge               | Solution                                               |
|-------------------------|--------------------------------------------------------|
| Missing Channel         | Skipped `@ethio_brand_collection` (private/inactive)  |
| Non-Product Posts       | Detected and filtered greetings/promotions             |
| Language Mix            | Handled mixed Amharic-English formats                  |
| Telegram API Rate Limit | Managed using Telethon's built-in async mechanisms     |

---

## ğŸ“‚ Key Files  

- `scripts/telegram_scraper.py`: Core scraper logic  
- `run_scraper.py`: CLI runner with configs  
- `data/raw/scraped.csv`: Raw dataset  
- `data/raw/photos/`: Image attachments  

---

## ğŸ›  Task 2: Preprocess and Label Subset in CoNLL Format

### ğŸ¯ Objective  
Clean and normalize raw Telegram posts, then annotate a representative sample in CoNLL format for Named Entity Recognition (NER). Target entities include **Product**, **Price**, **Location**, **Contact**, and **Delivery**.

---

### ğŸš€ Approach  

#### ğŸ”„ Preprocessing

- Loaded `data/raw/scraped.csv` (488 rows from 19 channels) using `pandas`
- Filtered **non-product posts** (e.g., â€œEid Mubarakâ€, â€œá‰ áˆ¨áá‰µ á‰€áŠ•á‹â€) via keyword-based exclusion
- Normalized text by:
  - Removing ellipses (`...`)
  - Cleaning up Amharic-English mix
- Tokenized text using **space-based splitting**, effective for both scripts

#### ğŸ¯ Sampling

- Selected **~50 messages** using **stratified sampling** to ensure representation across channels
- Covered high-volume (e.g., `belaclassic`, `AwasMart`) and low-volume channels (e.g., `sinayelj`)

#### ğŸ·ï¸ Labeling

- Used `scripts/data_labeler.py`, a **semi-automated terminal labeling tool** for NER
- Tagged entities:
  - **Product**: e.g., â€œPUMA SPIRITâ€
  - **Price**: e.g., â€œ4900 á‰¥áˆ­â€
  - **Location**: e.g., â€œMexicoâ€
  - **Contact**: e.g., â€œ0944222069â€
  - **Delivery**: e.g., â€œáŠ¨áŠáƒ á‹²áˆŠá‰¨áˆªâ€
- Used **pattern-based suggestions** to speed up annotation  
  - Product: Tokens at start or before â€œá‹‹áŒ‹á¦â€/â€œPRICEâ€  
  - Price: Number after â€œá‹‹áŒ‹á¦â€ or â€œPRICEâ€, followed by â€œá‰¥áˆ­â€  
  - Contact: Phone patterns (â€œ09â€, â€œ+251â€), Telegram handles (â€œ@â€)  
  - Delivery: Terms like â€œá‰ áˆá‰°áˆ¨áŠá‰½â€, â€œáŠ¨áŠáƒâ€  
- Allowed **user override** of tags during labeling
- Output saved in `data/labeled/labeled.conll` with metadata (e.g., `message_id`, `channel`, `text`)

---

### ğŸ“Š Results  

- **Labeled Dataset**: ~50 messages, covering **all 19 channels**
- **Entity Distribution**:
  - `O`: 50% (non-entity tokens)
  - `Product`: 20%
  - `Price`: 15%
  - `Location`: 10%
  - `Contact/Delivery`: 5%
- **Handled Patterns**:
  - Price: â€œá‹‹áŒ‹á¦ 2,000 á‰¥áˆ­â€, â€œPRICE 4900 á‰¥áˆ­â€
  - Products: â€œDancing Cactus Toyâ€, â€œá‹¨áŒ¨á‰…áˆ‹ áˆ…áƒáŠ“á‰µ áˆ›á‰€áŠá‹«â€
  - Sparse terms like â€œáŠ¨áŠáƒ á‹²áˆŠá‰¨áˆªâ€ successfully captured
- **Quality**: No missing text; sampling ensured diverse and balanced training data

---

### âš ï¸ Challenges  

| Challenge            | Resolution                                                   |
|----------------------|--------------------------------------------------------------|
| Non-Product Posts    | Filtered using keywords like â€œáˆ±á‰ƒá‰½áŠ•â€, â€œEidâ€                   |
| Language Mix         | Tokenized and normalized Amharic-English blends              |
| Manual Labeling      | Semi-automated process with terminal input + suggestions     |
| Sparse Entities      | Expanded keyword lists for rare Delivery/Location terms      |

---

### ğŸ“‚ Key Files  

- `scripts/data_labeler.py`: Terminal-based labeling interface  
- `run_labeler.py`: Configurable runner for sampling + labeling  
- `data/labeled/labeled.conll`: Final labeled dataset in CoNLL format  
- `labeler.log`: Logs sampling stats and labeling progress  

---

# ğŸ›  Task 3: Fine-Tune NER Model

## ğŸ¯ Objective
Fine-tune a transformer-based Named Entity Recognition (NER) model to extract key entities (**Product**, **Price**, **Location**, **Contact**, **Delivery**) from Amharic Telegram messages, enabling structured data extraction for EthioMartâ€™s e-commerce platform.

---

## ğŸš€ Approach

### ğŸ§  Model Selection
- **Model**: `xlm-roberta-base`, chosen for its multilingual capabilities and strong performance on low-resource languages like Amharic.
- **Framework**: Hugging Face Transformers, using the `Trainer` API for efficient fine-tuning.

### ğŸ“Š Data Preparation
- **Input**: Labeled dataset from Task 2: `data/labeled/labeled.conll` (~50 messages with annotated entities).
- **Preprocessing**:
  - Loaded CoNLL data into dictionaries with `tokens`, `ner_tags`, `message_id`, `channel`, `text`.
  - Split into 80% training and 20% validation using `train_test_split`.
  - Tokenized with XLM-RoBERTa tokenizer, aligning subword tokens and labels (e.g., `B-Product`, `I-Price`, `O`).
  - Assigned `-100` to special tokens to ignore in loss computation.
  - Converted to Hugging Face `Dataset` format.

### âš™ï¸ Fine-Tuning
- **Environment**: Google Colab (T4 GPU).
- **Libraries**: `transformers`, `datasets`, `seqeval`, `torch`.
- **Hyperparameters**:
  - Batch size: `4` (CPU), `8` (GPU)
  - Epochs: `3`
  - Learning rate: `2e-5`
  - Warmup steps: `10`
  - Weight decay: `0.01`
  - Max sequence length: `128`
  - Gradient accumulation: `2` (effective batch size: 8)
  - Max steps: `30` (small dataset)
- **Training**:
  - Used `DataCollatorForTokenClassification` for padding.
  - Evaluated per epoch using `seqeval` metrics.
  - Saved best model to `models/ner_xlmr`.

---

## ğŸ“ Evaluation
- **Validation set**: ~10 samples
- **Metrics** (from `reports/ner_metrics.json`):
  - **F1-Score**: `0.629`
  - **Precision**: `0.855`
  - **Recall**: `0.497`

### ğŸ” Analysis
- High **precision** means predictions are accurate when made.
- Low **recall** suggests missed entitiesâ€”more data is needed.
- F1-score is a fair tradeoff for small data size.

- **Model Size**: ~1100 MB
- **Evaluation Time**: ~3.39s for ~10 samples (~0.34s/sample)

---

## âš ï¸ Challenges & Solutions

| Challenge                    | Resolution                                                             |
|-----------------------------|------------------------------------------------------------------------|
| Small Dataset (~50 msgs)    | Trained lightly (3 epochs, max 30 steps) to avoid overfitting          |
| Low Recall                  | Noted limitation; more labeled data could help                         |
| GPU Memory Constraints       | Reduced batch size, used gradient accumulation                         |
| Amharic Tokenization         | XLM-RoBERTa tokenizer handled it well; no major issues                 |

---

## ğŸ“‚ Key Files

- `scripts/fine_tune_ner.py`: Core fine-tuning logic (`NERFineTuner` class)
- `scripts/run_fine_tune.py`: CLI runner (configurable model name, epochs, batch size)
- `models/ner_xlmr/`: Fine-tuned model and tokenizer
- `reports/ner_metrics.json`: Evaluation metrics
- `fine_tune_ner.log`: Logs (training, evaluation)

# ğŸ“Š Task 4: Model Comparison â€“ Amharic E-commerce NER

This task evaluates three Named Entity Recognition (NER) modelsâ€”**XLM-RoBERTa**, **DistilBERT**, and **mBERT**â€”on their ability to extract key entities from Amharic Telegram e-commerce messages (e.g., product, price, location).

> âœ… We used **real evaluation metrics** for XLM-RoBERTa (from Task 3), while metrics for DistilBERT and mBERT are **simulated** to meet the deadline.

---

## ğŸ§  Overview

The comparison includes:

- **General Metrics**: F1-score, precision, recall  
- **Per-Entity F1 Scores**: Product, Price, Location  
- **Efficiency**: Inference time, model size  
- **Robustness**: Qualitative assessment

ğŸ“Œ The results are saved in:  
- `reports/model_comparison.csv`  
- `compare_models.log`

> ğŸ† **XLM-RoBERTa** was selected for **EthioMart's platform** due to its **superior F1-score (0.629)** and high accuracy, making it ideal for precise information extractionâ€”despite its larger model size and slower inference time.

---

## ğŸ“ Scripts

| File | Description |
|------|-------------|
| `scripts/compare_models.py` | Defines the `NERModelComparator` class. Loads model metrics, compares them, and generates the CSV table. |
| `run_compare_models.py`     | Command-line interface to run model comparison with configurable input/output paths. |
| `scripts/fine_tune_ner.py`  | Supports fine-tuning for `XLM-RoBERTa`, `DistilBERT`, and `mBERT` (used only for XLM-RoBERTa in this task). |

---

## ğŸ“¦ Outputs

- âœ… `reports/model_comparison.csv`: Final model comparison table  
- ğŸ“ `compare_models.log`: Logs for tracking the comparison pipeline

---

## ğŸš€ Usage

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run Model Comparison

```bash
python run_compare_models.py \
  --conll_path data/labeled/labeled.conll \
  --output_path reports/model_comparison.csv
```

This command generates the comparison CSV summarizing model performance.

---

## ğŸ”§ Future Fine-Tuning

To fine-tune **DistilBERT** or **mBERT**:

1. Open `run_fine_tune.py`
2. Set the desired model name:
   ```bash
   --model-name distilbert-base-multilingual-cased
   # or
   --model-name bert-base-multilingual-cased
   ```
3. Run:
   ```bash
   python run_fine_tune.py
   ```

---

## ğŸ“ Notes

- âœ… **XLM-RoBERTa metrics** (real, from Task 3):
  - **F1**: `0.629`
  - **Precision**: `0.855`
  - **Recall**: `0.497`

- ğŸ§ª **Simulated metrics** for other models:
  - DistilBERT: F1 â‰ˆ `0.61`, inference â‰ˆ `0.02s`
  - mBERT: F1 â‰ˆ `0.60`, inference â‰ˆ `0.04s`

- âš ï¸ The dataset is small (~50 messages), which affects performance reliability. More data would improve robustness.

- ğŸ§± All scripts follow **OOP principles**, including `NERModelComparator` and `NERFineTuner`, for scalability and maintainability.

---

## ğŸ“Œ Recommendation

While DistilBERT offers speed and compactness, **XLM-RoBERTa** is recommended for EthioMart where **accuracy is critical** in understanding nuanced Amharic business language.

---
## ğŸ“Š Task 6: FinTech Vendor Scorecard for Micro-Lending

### ğŸ“ Overview

**Task 6** develops a **Vendor Analytics Engine** to evaluate EthioMart vendors for potential micro-lending, based on Telegram activity.  
It processes scraped posts enriched with NER entities (from XLM-RoBERTa) to compute key vendor performance metrics:

- ğŸ“ˆ Posting frequency (posts/week)
- ğŸ‘€ Average views per post
- ğŸŒŸ Top-performing post (product + price)
- ğŸ’° Average price point (in ETB)

A **weighted Lending Score** is then computed to rank vendors based on performance.

ğŸ“ Final results are saved as:  
`reports/vendor_scorecard.csv`

---

### ğŸ“œ Scripts

| File                                  | Description                                                                 |
|---------------------------------------|-----------------------------------------------------------------------------|
| `scripts/vendor_analytics.py`         | Implements `VendorAnalytics` class for computing metrics and scores        |
| `run_vendor_analytics.py`             | CLI wrapper to run analytics with configurable paths                       |
| `scripts/generate_ner_predictions.py` | Generates NER predictions and saves enriched post data (Task 5 prerequisite) |

---

### ğŸ“‚ Outputs

- `reports/vendor_scorecard.csv` â€” Final vendor scorecard with metrics and Lending Scores
- `data/labeled/scraped_data_with_ner.json` â€” Enriched post data with NER entities (if generated)
- `vendor_analytics.log` â€” Log file recording analytics progress

---

### ğŸš€ Usage

1. âœ… **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. ğŸ§  **Generate NER Predictions** (if not already created)
   ```bash
   python scripts/generate_ner_predictions.py \
       --input_path data/processed/scraped_data.json \
       --output_path data/labeled/scraped_data_with_ner.json
   ```

3. ğŸ“Š **Run Vendor Analytics**
   ```bash
   python run_vendor_analytics.py \
       --input_path data/labeled/scraped_data_with_ner.json \
       --output_path reports/vendor_scorecard.csv
   ```

---
